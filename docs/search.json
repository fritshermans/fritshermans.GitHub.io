[
  {
    "objectID": "posts/PyMinhash.html",
    "href": "posts/PyMinhash.html",
    "title": "Finding duplicate records using PyMinHash",
    "section": "",
    "text": "MinHashing is a very efficient way of finding similar records in a dataset based on Jaccard similarity. My Python package PyMinHash implements efficient minhashing for Pandas dataframes. In this post I explain how to use PyMinHash. You can find more information on how the minhashing algorithm works here."
  },
  {
    "objectID": "posts/PyMinhash.html#installation",
    "href": "posts/PyMinhash.html#installation",
    "title": "Finding duplicate records using PyMinHash",
    "section": "Installation",
    "text": "Installation\nInstall PyMinHash from Pypi in the command line as follows:\npip install pyminhash"
  },
  {
    "objectID": "posts/PyMinhash.html#apply-on-pandas-dataframe",
    "href": "posts/PyMinhash.html#apply-on-pandas-dataframe",
    "title": "Finding duplicate records using PyMinHash",
    "section": "Apply on Pandas dataframe",
    "text": "Apply on Pandas dataframe\nPyMinHash comes with a toy dataset containing various name and address combinations of Stoxx50 companies. Let’s load the data and see what it contains.\n\nfrom pyminhash.datasets import load_data\ndf = load_data()\n\n\n\n\n\n\nname\n\n\n\n\nadidas ag adi dassler strasse 1 91074 germany\n\n\nadidas ag adi dassler strasse 1 91074 herzogenaurach\n\n\nadidas ag adi dassler strasse 1 91074 herzogenaurach germany\n\n\nairbus se 2333 cs leiden netherlands\n\n\nairbus se 2333 cs netherlands\n\n\n\n\n\nWe’re going to match various representations that belong to the same company. For this, we import create a MinHash object and tell it to use 10 hash tables. More hash tables means more accurate Jaccard similarity calculation but also requires more time and memory.\n\nfrom pyminhash.pyminhash import MinHash\nmyHasher = MinHash(n_hash_tables=10)\n\nThe fit_predict method needs the dataframe and the name of the column to which minhashing should be applied. The result is a dataframe containing all pairs that have a non-zero Jaccard similarity:\n\nresult = myHasher.fit_predict(df, 'name')\n\n\n\n\n\n\nname_1\nname_2\njaccard_sim\n\n\n\n\nengie sa 1 place samuel de champlain 92400 courbevoie\nengie sa 1 place samuel de champlain 92400 france\n1.0\n\n\nkoninklijke philips n v amstelplein 2 1096 bc\nkoninklijke philips n v amstelplein 2 1096 bc amsterdam\n1.0\n\n\nasml holding n v de run 6501 5504 dr veldhoven netherlands\nasml holding n v de run 6501 veldhoven netherlands\n1.0\n\n\namadeus it group s a salvador de madariaga 1 28027 madrid\namadeus it group s a salvador de madariaga 1 28027 madrid spain\n1.0\n\n\ndeutsche telekom ag 53113 bonn germany\ndeutsche telekom ag 53113 germany\n1.0\n\n\n\n\n\nAs one can see below, for a Jaccard similarity of 1.0, all words in the shortest string appear in the longest string. For lower Jaccard similarity values, the match is less than perfect. Note that Jaccard similarity has granularity of 1/n_hash_tables, in this example 0.1.\n\n\n\n\n\nname_1\nname_2\njaccard_sim\n\n\n\n\nengie sa 1 place samuel de champlain 92400 courbevoie\nengie sa 1 place samuel de champlain 92400 france\n1.0\n\n\nkoninklijke philips n v amstelplein 2 1096 bc\nkoninklijke philips n v amstelplein 2 1096 bc amsterdam\n1.0\n\n\nvinci sa 1 cours ferdinand de lesseps 92851 france\nvinci sa 1 cours ferdinand de lesseps rueil malmaison france\n0.9\n\n\nsanofi 54 rue la boetie 75008 france\nsanofi 54 rue la boetie 75008 paris france\n0.9\n\n\nairbus se 2333 cs leiden netherlands\nairbus se 2333 cs netherlands\n0.8\n\n\nfresenius se co kgaa else kroner strasse 1 61352 bad homburg vor der hohe germany\nfresenius se co kgaa else kroner strasse 1 61352 germany\n0.8\n\n\nbayerische motoren werke aktiengesellschaft munich germany\nbayerische motoren werke aktiengesellschaft petuelring 130 80788 munich\n0.7\n\n\nbayerische motoren werke aktiengesellschaft munich germany\nbayerische motoren werke aktiengesellschaft petuelring 130 munich germany\n0.7\n\n\naxa sa 75008 paris france\nsanofi 54 rue la boetie 75008 paris france\n0.6\n\n\nbanco bilbao vizcaya argentaria s a 48005 bilbao spain\nbanco bilbao vizcaya argentaria s a bilbao\n0.6\n\n\nlvmh moet hennessy louis vuitton societe europeenne 75008 france\nsanofi 54 rue la boetie 75008 paris france\n0.5\n\n\nsafran sa 2 boulevard du general martial valin paris france\nsafran sa 75724 france\n0.5\n\n\nmunchener ruckversicherungs gesellschaft aktiengesellschaft koniginstrasse 107 munich germany\nsiemens aktiengesellschaft munich germany\n0.4\n\n\nallianz se koniginstrasse 28 munich germany\nmunchener ruckversicherungs gesellschaft aktiengesellschaft koniginstrasse 107 munich germany\n0.4\n\n\nbnp paribas sa 16 boulevard des italiens 75009 france\nsociete generale societe 75009 paris france\n0.3\n\n\nessilorluxottica 1 6 rue paul cezanne paris france\nl oreal s a 92117 france\n0.3\n\n\nbnp paribas sa paris\nengie sa 92400 courbevoie france\n0.2\n\n\nessilorluxottica 1 6 rue paul cezanne paris france\nschneider electric s e 92500 france\n0.2\n\n\nsociete generale societe 75009 paris france\ntotal s a 2 place jean millier paris france\n0.1\n\n\nvinci sa 1 cours ferdinand de lesseps rueil malmaison france\nvivendi sa 75380 paris\n0.1\n\n\n\n\n\nIf you increase the number of hash tables, the accuracy of the Jaccard similarity increases but it comes at the cost of speed and memory usage."
  },
  {
    "objectID": "posts/hard_to_classify_datapoints.html",
    "href": "posts/hard_to_classify_datapoints.html",
    "title": "Hard-to-classify datapoints in imbalanced data problems",
    "section": "",
    "text": "Classification for imbalanced data problems is hard; it requires paying attention to balancing classes when training and choosing the right model performance metric. Many articles are written about imbalanced classification and packages containing metrics and sampling techniques are open sourced. One important aspect on classification for imbalanced data problems is given insufficient attention; ‘instance hardness’ and cross validation.\nThe term instance hardness is used in literature to express the difficulty to correctly classify an instance. An instance could be hard to classify because its neighbors in the feature space have a different label, the features just don’t bear the information to correctly predict the class for such instance. We see these cases a lot in the field of financial crime detection; all features for a particular transaction indicate riskyness but in reality the transaction was just legit or vice versa. The literature about instance hardness describes how to measure it and how to use these measures for resampling training data and thereby removing hard-to-classify instances to improve test set performance.\nThe way hard-to-classify instances are distributed over train and test sets (e.g. in hyperparameter tuning or feature selection using cross validation) has significant effect on the test set performance metrics. A few instances with large instance hardness can change results significantly. In this blog I will explain this problem and show a solution to limit its effect on cross validation metric variance.\n\n\n\n\n\nFigure 1: Instance hardness example"
  },
  {
    "objectID": "posts/hard_to_classify_datapoints.html#instance-hardness",
    "href": "posts/hard_to_classify_datapoints.html#instance-hardness",
    "title": "Hard-to-classify datapoints in imbalanced data problems",
    "section": "",
    "text": "Classification for imbalanced data problems is hard; it requires paying attention to balancing classes when training and choosing the right model performance metric. Many articles are written about imbalanced classification and packages containing metrics and sampling techniques are open sourced. One important aspect on classification for imbalanced data problems is given insufficient attention; ‘instance hardness’ and cross validation.\nThe term instance hardness is used in literature to express the difficulty to correctly classify an instance. An instance could be hard to classify because its neighbors in the feature space have a different label, the features just don’t bear the information to correctly predict the class for such instance. We see these cases a lot in the field of financial crime detection; all features for a particular transaction indicate riskyness but in reality the transaction was just legit or vice versa. The literature about instance hardness describes how to measure it and how to use these measures for resampling training data and thereby removing hard-to-classify instances to improve test set performance.\nThe way hard-to-classify instances are distributed over train and test sets (e.g. in hyperparameter tuning or feature selection using cross validation) has significant effect on the test set performance metrics. A few instances with large instance hardness can change results significantly. In this blog I will explain this problem and show a solution to limit its effect on cross validation metric variance.\n\n\n\n\n\nFigure 1: Instance hardness example"
  },
  {
    "objectID": "posts/hard_to_classify_datapoints.html#area-under-precision-recall-curve",
    "href": "posts/hard_to_classify_datapoints.html#area-under-precision-recall-curve",
    "title": "Hard-to-classify datapoints in imbalanced data problems",
    "section": "Area Under Precision Recall Curve",
    "text": "Area Under Precision Recall Curve\nLet’s start by creating a dataset to work with. We create a dataset with 5% class imbalance using scikit-learn’s make_classification function and we make a train test split. Note that we set flip_y to zero to prevent labels from being randomly flipped.\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nrandom_state = 1\n\nX, y = make_classification(n_samples=5_000, \n                           weights=[0.95, 0.05],\n                           flip_y=0,\n                           random_state=random_state)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.3, \n                                                    stratify=y, \n                                                    random_state=random_state)\n\nThen we take a Random Forest classifier, fit it on the train data and make predictions on the test data. We ommit hyperparameter tuning because that wouldn’t serve explanation of this blog post topic. We plot the precision recall curve in Figure 2. Our classification model obtained a AUC-PR value of 0.67.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = (RandomForestClassifier(class_weight='balanced',\n                              random_state=random_state)\n       .fit(X_train, y_train))\n\nprobas = clf.predict_proba(X_test)[:,1]\n\ndf = pd.DataFrame({'y':y_test, 'proba':probas})\n\n\n\n\n\n\nFigure 2: Precision recall curve\n\n\n\n\nNow we are going to flip the label of just one sample in our test set to see its effect on AUC-PR. We change the label of the sample with label 1 and the largest score (given by our random forest) from 1 to 0. Table 1 (a) and Table 1 (b) show how this changes the samples with highest scores in the data used for AUC-PR calculation. Figure 3 (a) and Figure 3 (b) show the precision-recall curve before and after the flip. As you can see, only flipping the label for this single point reduces the AUC-PR from 0.67 to 0.62. If we do this another time (Table 1 (c) and Figure 3 (c)), the AUC-PR drops further to 0.58. This shows how strong few samples with large instance hardness affect AUC-PR.\n\ndef flip_label_highest_score(df):\n    idx_to_change = df[df['y']==1].sort_values('proba').index[-1]\n    df.loc[idx_to_change, 'y'] = 0\n    return df\n\n\ndf = flip_label_highest_score(df)\n\n\n\nTable 1: Largest scores\n\n\n\n\n(a)\n\n\ny\nproba\n\n\n\n\n1\n0.90\n\n\n1\n0.86\n\n\n1\n0.84\n\n\n1\n0.78\n\n\n1\n0.78\n\n\n\n\n\n\n(b)\n\n\ny\nproba\n\n\n\n\n0\n0.90\n\n\n1\n0.86\n\n\n1\n0.84\n\n\n1\n0.78\n\n\n1\n0.78\n\n\n\n\n\n\n(c)\n\n\ny\nproba\n\n\n\n\n0\n0.90\n\n\n0\n0.86\n\n\n1\n0.84\n\n\n1\n0.78\n\n\n1\n0.78\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) As is\n\n\n\n\n\n\n\n(b) After flipping label of point with largest score from 1 to 0\n\n\n\n\n\n\n\n(c) After flipping the next point\n\n\n\n\nFigure 3: Precision recall curves"
  },
  {
    "objectID": "posts/hard_to_classify_datapoints.html#instance-hardness-and-cross-validation",
    "href": "posts/hard_to_classify_datapoints.html#instance-hardness-and-cross-validation",
    "title": "Hard-to-classify datapoints in imbalanced data problems",
    "section": "Instance hardness and cross validation",
    "text": "Instance hardness and cross validation\nThis blog post is not a complaint about AUC-PR; this model performance metric is well suited for imbalanced data problems just because it is sensitive for model performance for the minority class. The effect of a few points can become large when we make train-test splits for example in cross validation when doing hyperparameter tuning. If multiple hard-to-classifiy samples end up in the same fold, the AUC-PR value for that fold will be significanty lower as we have seen above. Let’s show this with the following experiment.\nFor this experiment, we use our train set and we create out of sample model scores using scikit-learn’s cross_val_predict function and a random forest classifier without hyperparameter tuning. As earlier, we flip the label for positive samples with the largest model score. We do this for 10 samples. Then we obtain cross validation AUC-PR values using cross_val_score with 3 folds. To show how randomness affects in which folds the hard-to-classify samples end up, we do this experiment 5 times. Table 2 shows how the AUC-PR scores differ over the folds. This potentially impacts hyperparameter tuning as hyperparameter combinations that are accidentically tested on a test fold without hard-to-classify samples will be favoured.\n\n\n\n\nTable 2: Cross validation AUC-PR scores for different runs\n\n\nrun\ncv fold 1\ncv fold 2\ncv fold 3\nmax diff\n\n\n\n\n1\n0.503364\n0.417450\n0.457315\n0.085914\n\n\n2\n0.425198\n0.522625\n0.486998\n0.097427\n\n\n3\n0.386822\n0.481074\n0.627011\n0.240189\n\n\n4\n0.397935\n0.563807\n0.508088\n0.165872\n\n\n5\n0.510637\n0.553232\n0.475470\n0.077762\n\n\n\n\n\n\nThe goal is now to make sure these hard-to-classify samples are equally distributed over the folds. To that end, ‘instance hardness’ is defined as 1 minus the probability of the most probable class. In the case of binary classification:\n\nH(x)=1-P(\\hat{y}|x)\n\nIn this equation H(x) is the instance hardness for a sample with features x and P(\\hat{y}|x) the probability of predicted label \\hat{y} given the features. Note that this is the probability of the predicted label, not the output of predict_proba; if the model predicts label 0 and gives a predict_proba output of 0.1, the probability of label 0 is 0.9.\nNow we redo the exercise, but we create splits such that instance hardness is equally spread over the folds. We do this by calculating instance hardness (using cross_val_predict), sorting our data by instance hardness and creating cv groups by just adding a column containing the row number modulo cv, resulting in 0, 1, 2, 0, 1, 2 etc when cv=3. These values are then used as argument for groups in cross_val_score. To enable the usage of these groups, we need to use a cv splitter that takes groups into account, such as the StratifiedGroupKFold splitter. Table 3 shows how the out-of-sample AUC-PR values are much closer to each other.\n\n\n\n\nTable 3: Cross validation AUC-PR scores for different runs with equally distributed ‘hard-to-classify’ samples\n\n\nrun\ncv fold 1\ncv fold 2\ncv fold 3\nmax diff\n\n\n\n\n1\n0.475672\n0.508276\n0.529008\n0.053336\n\n\n2\n0.490002\n0.548628\n0.506362\n0.058626\n\n\n3\n0.473555\n0.488436\n0.534512\n0.060957\n\n\n4\n0.528816\n0.472013\n0.471103\n0.057714\n\n\n5\n0.518430\n0.480128\n0.461425\n0.057004"
  },
  {
    "objectID": "posts/hard_to_classify_datapoints.html#implementation-of-instancehardnesscv",
    "href": "posts/hard_to_classify_datapoints.html#implementation-of-instancehardnesscv",
    "title": "Hard-to-classify datapoints in imbalanced data problems",
    "section": "Implementation of InstanceHardnessCV",
    "text": "Implementation of InstanceHardnessCV\nLet’s now implement a scikit-learn CV splitter that does all the steps above. This cv splitter can be used for hyperparameter tuning, feature selection or any other scikit-learn object that has a cv argument.\n\nclass InstanceHardnessCV:\n    def __init__(self, n_splits=3, clf=None, random_state=None):\n        self.n_splits = n_splits\n        self.clf = clf\n        self.random_state = random_state\n        \n        \n    def split(self, X, y, groups=None):\n        df = pd.DataFrame(X)\n        features = df.columns\n        df['y'] = y\n        if self.clf is not None:\n            self.clf_ = self.clf\n        else:\n            self.clf_ = RandomForestClassifier(n_jobs=-1,\n                                               class_weight='balanced',\n                                               random_state=self.random_state)\n        df['proba'] = cross_val_predict(self.clf_, \n                                        df[features],\n                                        df['y'],\n                                        cv=self.n_splits,\n                                        method='predict_proba')[:,1]\n        df['hardness'] = abs(df['y']-df['proba'])\n        df = df.sort_values('hardness')\n        df['group'] = np.arange(len(df)) % self.n_splits\n        cv = StratifiedGroupKFold(n_splits=self.n_splits,\n                                  shuffle=True,\n                                  random_state=self.random_state)\n        for train_index, test_index in cv.split(df[features], df['y'], df['group']):\n            yield train_index, test_index\n        \n\n    def get_n_splits(self, X, y, groups=None):\n        return self.n_splits\n\nWhen we apply our InstanceHardnessCV splitter to our data with 10 artificially created ‘hard to classify’ datapoints, we get the results as presented before.\n\n\n\n\nTable 4: Cross validation AUC-PR scores for different runs with equally distributed ‘hard-to-classify’ samples using the InstanceHardnessCV splitter\n\n\nrun\ncv fold 1\ncv fold 2\ncv fold 3\nmax diff\n\n\n\n\n1\n0.529008\n0.475672\n0.508276\n0.053336\n\n\n2\n0.548628\n0.506362\n0.490002\n0.058626\n\n\n3\n0.534512\n0.473555\n0.488436\n0.060957\n\n\n4\n0.528816\n0.472013\n0.471103\n0.057714\n\n\n5\n0.518430\n0.480128\n0.461425\n0.057004\n\n\n\n\n\n\nOne can use a different classifier for calculating instance hardness than the random forest that we set as default. Note that the purpose of this classifier is to identify the difficult samples; the data points for which features suggest a positive label but the ground truth is a negative label. This is the reason why we didn’t pay attention to hyperparameter tuning."
  },
  {
    "objectID": "posts/taxonomy_encoder_blog.html",
    "href": "posts/taxonomy_encoder_blog.html",
    "title": "Taxonomy feature encoding",
    "section": "",
    "text": "Features like zipcodes or industry codes (NAICS, MCC) contain information that is part of a taxomy. Although these feature values are numerical, it doesn’t necessarily make sense to use them as ordinal features; a region’s zipcode might be a higher value than another region’s zipcode, that doesn’t mean that there is valuable ranking in these values. If we encode taxonomy bearing features using One-Hot-Encoding, the number of features blows up tremendously. Moreover, we loose helpful information on the similarity of adjacent values. E.g. the MCC industry codes for ‘Commerical clothing’ (5137) and ‘Commerical footwear’ (5139) are clearly more similar than for example ‘Child Care services’ (8351). We could overcome this issue by One-Hot-Encoding at a higher level (e.g. 5xxx for ‘Stores’ and 8xxx for ‘Professional Services and Membership Organizations’) but dependent on the modelling task, we might want to have higher granularity in specific parts of the possible feature value range.\nTo overcome these issues, I created the Taxonomy Encoder for scikit-learn. Before going to the implementation, let’s first have a look at a practical application; house price prediction in the Netherlands."
  },
  {
    "objectID": "posts/taxonomy_encoder_blog.html#introduction",
    "href": "posts/taxonomy_encoder_blog.html#introduction",
    "title": "Taxonomy feature encoding",
    "section": "",
    "text": "Features like zipcodes or industry codes (NAICS, MCC) contain information that is part of a taxomy. Although these feature values are numerical, it doesn’t necessarily make sense to use them as ordinal features; a region’s zipcode might be a higher value than another region’s zipcode, that doesn’t mean that there is valuable ranking in these values. If we encode taxonomy bearing features using One-Hot-Encoding, the number of features blows up tremendously. Moreover, we loose helpful information on the similarity of adjacent values. E.g. the MCC industry codes for ‘Commerical clothing’ (5137) and ‘Commerical footwear’ (5139) are clearly more similar than for example ‘Child Care services’ (8351). We could overcome this issue by One-Hot-Encoding at a higher level (e.g. 5xxx for ‘Stores’ and 8xxx for ‘Professional Services and Membership Organizations’) but dependent on the modelling task, we might want to have higher granularity in specific parts of the possible feature value range.\nTo overcome these issues, I created the Taxonomy Encoder for scikit-learn. Before going to the implementation, let’s first have a look at a practical application; house price prediction in the Netherlands."
  },
  {
    "objectID": "posts/taxonomy_encoder_blog.html#house-price-prediction",
    "href": "posts/taxonomy_encoder_blog.html#house-price-prediction",
    "title": "Taxonomy feature encoding",
    "section": "House price prediction",
    "text": "House price prediction\nWhen predicting house prices, zipcode is an example of a taxonomy bearing feature where we need different granularity for encoding the feature in different regions. Figure 1 shows average house prices in the Netherlands in 2023 per zipcode (source: CBS). We know that in cities house prices in different zones differ a lot, even if the zones are only a few kilometers away. In more rural areas, these differences are often much less prevalent. To illustrate this, Figure 2 zooms in on Amsterdam (a) and the province of Limburg (b). Amsterdam is a relatively small city that has zones with house prices in the lower end and the most expensive houses in the country. The province of Limburg is a much larger area but has significantly less variation in house prices. Going back to our aim of encoding the zipcode feature; we need different granularity for cities than for the country side. The question is how to choose this granularity.\n\n\n\n\n\nFigure 1: House price per zip code in the Netherlands\n\n\n\n\n\n\n\n\n\n\n\n(a) City of Amsterdam\n\n\n\n\n\n\n\n(b) Province of Limburg\n\n\n\n\nFigure 2: House price average per zip code, note the difference in price homogeneity in cities vs rural areas\n\n\nLet’s use a decision tree regressor to create segments of zipcodes that are homogenous with respect to mean house prices. As I’m lacking a dataset with house prices of individual houses, I’m going to create such dataset by concatenating the CBS dataset 10 times and multiply house prices with a random factor between 0.9 and 1.1 to introduce some variation. The decision tree regressor is fitted with max_leaf_nodes set to 50. This means that the zipcodes will be placed in 50 segments. To illustrate the effectiveness of this method, I show in-sample predictions for the most expensive (Table 1) and the least expensive areas (Table 2). The two tables show encoded mean house prices, the range of zipcodes, the number of zipcodes in that range (apparently not all possible values are used as zipcodes!) and the cities where these zipcodes are in. Clearly, the most expensive areas are much smaller and require a higher granularity of zipcode encoding than areas with lower house prices. Note how Amsterdam has even three distinct zipcode areas in the country top 10. If we use these in-sample generated zipcode encodings in our model, we would make the mistake of information leakage. The house price of each house would be used to generate a feature that is used to predict that same house price. This is where the Taxonomy Encoder comes into play.\n\n\n\n\n\n\n\nTable 1: Encoded zip codes for areas with most expensive houses\n\n\n\nzipcode\ncity\n\n\n\nmin\nmax\nnunique\nunique\n\n\nencoded_mean\n\n\n\n\n\n\n\n\n1,340,222\n2243\n2244\n2\n[Wassenaar]\n\n\n1,276,450\n2111\n2111\n1\n[Aerdenhout]\n\n\n1,221,419\n1077\n1077\n1\n[Amsterdam]\n\n\n1,138,075\n1358\n1358\n1\n[Almere]\n\n\n1,015,257\n1071\n1071\n1\n[Amsterdam]\n\n\n995,357\n3546\n3546\n1\n[Utrecht]\n\n\n881,411\n2051\n2061\n2\n[Overveen, Bloemendaal]\n\n\n802,266\n1026\n1028\n3\n[Amsterdam]\n\n\n747,447\n2106\n2106\n1\n[Heemstede]\n\n\n691,614\n1251\n1272\n6\n[Laren, Blaricum, Huizen]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Encoded zip codes for areas with least expensive houses\n\n\n\nzipcode\ncity\n\n\n\nmin\nmax\nnunique\nunique\n\n\nencoded_mean\n\n\n\n\n\n\n\n\n205,692\n6369\n6511\n43\n[Heerlen, Simpelveld, Landgraaf, nan, Hoensbroek, Amstenrade, Oirsbeek, Doenrade, Brunssum, Merkelbeek, Schinveld, Jabeek, Bingelrade, Kerkrade, Eygelshoven, Nijmegen]\n\n\n234,407\n3066\n3119\n30\n[Rotterdam, Schiedam]\n\n\n244,079\n2511\n2547\n24\n[Den Haag]\n\n\n247,141\n9541\n9999\n254\n[Groningen, Lauwersoog, Hornhuizen, Vlagtwedde, Bourtange, Sellingen, Ter Apel, Ter Apelkanaal, Zandberg, Veelerveen, 2e Exloërmond, 1e Exloërmond, Exloërveen, Musselkanaal, Mussel, Vledderveen, O...\n\n\n269,679\n4331\n4707\n164\n[Terneuzen, Nieuw Namen, Kloosterzande, Steenbergen, Oud-Vossemeer, Middelburg, Nieuw- en Sint Joosland, Arnemuiden, Veere, Gapinge, Serooskerke, Vrouwenpolder, Oostkapelle, Domburg, Westkapelle, ...\n\n\n281,335\n8574\n9301\n336\n[Workum, Oosterend, Grou, Drachten, Bakhuizen, Elahuizen, Oudega, Kolderwolde, Hemelum, Sneek, Gaastmeer, Idzega, Sandfirden, Blauwhuis, Westhem, Abbega, Oosthem, Heeg, Hommerts, Jutrijp, Uitwelli...\n\n\n283,546\n5851\n6367\n218\n[Susteren, Geleen, Eys, Afferden L, Siebengewald, Bergen L, Well L, Wellerlooi, Wanssum, Geijsteren, Blitterswijck, Meerlo, Tienray, Swolgen, Broekhuizenvorst, Broekhuizen, Venlo, Tegelen, Steyl, ...\n\n\n301,971\n3121\n3443\n152\n[Hellevoetsluis, Numansdorp, Dordrecht, Schiedam, Vlaardingen, Maassluis, Hoek van Holland, Maasland, Rhoon, Poortugaal, Rozenburg, Hoogvliet Rotterdam, Pernis Rotterdam, Spijkenisse, Hekelingen, ...\n\n\n307,961\n3551\n3565\n10\n[Utrecht]\n\n\n317,922\n1273\n1357\n39\n[Almere, Huizen]"
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#introduction",
    "href": "posts/hyperparameter_tuning_spark.html#introduction",
    "title": "Distributed hyperparameter tuning of Scikit-learn models in Spark",
    "section": "Introduction",
    "text": "Introduction\nHyperparameter tuning of machine learning models often requires significant computing time. Scikit-learn implements parallel processing to speed things up, but real speed gain can only be achieved by applying distributed computing like using Spark. In this blog post I show how to do hyperparameter tuning in Spark for any machine learning model, independent whether it’s Scikit-learn, Tensorflow/Keras, XGBoost, LightGBM etc."
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#create-combinations-of-hyperparameter-values",
    "href": "posts/hyperparameter_tuning_spark.html#create-combinations-of-hyperparameter-values",
    "title": "Distributed hyperparameter tuning of Scikit-learn models in Spark",
    "section": "Create combinations of hyperparameter values",
    "text": "Create combinations of hyperparameter values\nFirst, we’re going to create hyperparameter combinations that we want to test our model for. Below is a helper function to create all combinations for a param_grid that contains the arguments and the values to test for.\n\nimport numpy as np\nfrom itertools import product\n\n\ndef create_hyperparameter_combinations(param_grid):\n    combinations = list(product(*param_grid.values()))\n    return [dict(zip(param_grid.keys(), x)) for x in combinations]\n\nSo if our desired hyperparameter space is as follows…\n\nparam_grid = {'max_features': ['auto', 0.1, 0.3], 'min_samples_leaf': [None, 50]}\n\n…then the combination of these two hyperparameters and their values is obtained by:\n\ncreate_hyperparameter_combinations(param_grid)\n\n[{'max_features': 'auto', 'min_samples_leaf': None},\n {'max_features': 'auto', 'min_samples_leaf': 50},\n {'max_features': 0.1, 'min_samples_leaf': None},\n {'max_features': 0.1, 'min_samples_leaf': 50},\n {'max_features': 0.3, 'min_samples_leaf': None},\n {'max_features': 0.3, 'min_samples_leaf': 50}]"
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#create-evaluation-function",
    "href": "posts/hyperparameter_tuning_spark.html#create-evaluation-function",
    "title": "Distributed hyperparameter tuning of Scikit-learn models in Spark",
    "section": "Create evaluation function",
    "text": "Create evaluation function\nNow, we’re going to create a function that takes a single combination of hyperparameter values and returns performance metrics of the model with these hyperparameters for train and test data. This function will be evaluated in distributed fashion on our Spark cluster. If you want to test 500 different hyperparameter combinations, you will see 500 tasks being executed by Spark. In this example, we’re going to optimise hyperparameters for a Scikit-learn model. This requires Scikit-learn to be installed on the worker nodes of your Spark cluster.\nThe hyperparameter values for a particular combination are provided to the function as a json, so a string type. The advantage of this is that we don’t need to change the function definition if we want to add hyperparameters. Moreover, we are independent of the type of hyperparameter values. Many hyperparameters in Scikit-learn take different types like integers, floats and strings like for example max_features in the RandomForestClassifier. When we provide them as a string containing a json, Spark never complains.\nThe function definition is as follows, explanation continues below.\n\nimport json\n\nfrom sklearn.model_selection import cross_validate\n\ndef evaluate_clf(base_clf, hyperpars_json, X, y, cv=5):\n    hyperpars = json.loads(hyperpars_json)\n    base_clf.set_params(**hyperpars)\n    cv_results = cross_validate(base_clf, X, y, cv=cv, return_train_score=True)\n    return (hyperpars_json,\n            float(np.mean(cv_results['train_score'])),\n            float(np.mean(cv_results['test_score'])))\n\nIn the first line, we convert the single combination of hyperparameter values to a Python dict by using json.loads. The next line sets these parameters in our Scikit-learn model called base_clf. The Scikit-learn function cross_validate takes the model, our training data features X and target y to produce train and test scores using cross validation. This function returns the results in a dict of which we take the train_score and test_score values that we return along with the hyperpars_json that we entered. Since Spark sometimes has difficulties with the np.float64 type, we convert the scores to the float type."
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#distribute-the-evaluation-function-in-spark",
    "href": "posts/hyperparameter_tuning_spark.html#distribute-the-evaluation-function-in-spark",
    "title": "Distributed hyperparameter tuning of Scikit-learn models in Spark",
    "section": "Distribute the evaluation function in Spark",
    "text": "Distribute the evaluation function in Spark\nThe next step is to distribute the hyperparameter combinations and use our evaluation function to calculate model performance metrics for these hyperparameter values. This is done in the function below.\n\nfrom pyspark.sql import types as T\n\ndef get_hyperparameter_results(spark_session, base_clf, X, y, \n                               hyperpar_combinations, cv=5):\n    hyperpar_combinations_json = [json.dumps(x) for x in hyperpar_combinations]\n    hyperpars_rdd = spark_session.sparkContext.parallelize(hyperpar_combinations_json, \n                                                           len(hyperpar_combinations_json))\n\n    rdd_map_result = hyperpars_rdd.map(lambda x: evaluate_clf(base_clf, x, X, y, cv))\n\n    result_schema = T.StructType([T.StructField('hyperpars', T.StringType()),\n                                  T.StructField('mean_train_score', T.FloatType()),\n                                  T.StructField('mean_test_score', T.FloatType()),\n                                  ])\n\n    result_sdf = spark_session.createDataFrame(rdd_map_result, schema=result_schema)\n    result = (result_sdf.toPandas()\n              .sort_values('mean_test_score', ascending=False)\n              .reset_index(drop=True))\n\n    result['hyperpars'] = result['hyperpars'].apply(json.loads)\n    return result\n\nIn the first line we convert the dicts to json strings. Then, we parallelize these jsons by creating an RDD. The evaluate_clf is mapped to this RDD. The schema of the result is defined as a StructType containing StructFields. Then a Spark dataframe is created that hold the results of our hyperparameter tuning. We convert this Spark dataframe to a Pandas dataframe in order to explore the results easily. As a last step we convert the jsons back to dicts."
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#full-working-example",
    "href": "posts/hyperparameter_tuning_spark.html#full-working-example",
    "title": "Distributed hyperparameter tuning of Scikit-learn models in Spark",
    "section": "Full working example",
    "text": "Full working example\nIn the example below we do hyperparameter tuning of a DecisionTreeClassifier to predict classes for the famous iris dataset.\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\n\nX, y = load_iris(return_X_y=True)\n\nparam_grid = {'max_depth':[3,4,5,10], 'min_samples_leaf':[0.1, 5, 10]}\n\nhyperpar_combinations = create_hyperparameter_combinations(param_grid)\n\nresults = get_hyperparameter_results(spark, DecisionTreeClassifier(), X, y, \n                                     hyperpar_combinations, cv=5)\n\n\n\n\n\n\nhyperpars\nmean_train_score\nmean_test_score\n\n\n\n\n{'max_depth': 3, 'min_samples_leaf': 5}\n0.968333\n0.940000\n\n\n{'max_depth': 4, 'min_samples_leaf': 5}\n0.968333\n0.940000\n\n\n{'max_depth': 5, 'min_samples_leaf': 5}\n0.968333\n0.940000\n\n\n{'max_depth': 10, 'min_samples_leaf': 5}\n0.968333\n0.940000\n\n\n{'max_depth': 3, 'min_samples_leaf': 0.1}\n0.961667\n0.933333\n\n\n{'max_depth': 3, 'min_samples_leaf': 10}\n0.961667\n0.933333\n\n\n{'max_depth': 4, 'min_samples_leaf': 0.1}\n0.961667\n0.933333\n\n\n{'max_depth': 4, 'min_samples_leaf': 10}\n0.961667\n0.933333\n\n\n{'max_depth': 5, 'min_samples_leaf': 0.1}\n0.961667\n0.933333\n\n\n{'max_depth': 5, 'min_samples_leaf': 10}\n0.961667\n0.933333\n\n\n{'max_depth': 10, 'min_samples_leaf': 0.1}\n0.961667\n0.933333\n\n\n{'max_depth': 10, 'min_samples_leaf': 10}\n0.961667\n0.933333"
  },
  {
    "objectID": "posts/aggregation_learning.html",
    "href": "posts/aggregation_learning.html",
    "title": "Aggregation learning",
    "section": "",
    "text": "For some modelling exercises, input data is at a different granularity level than the target.\nOne example is assigning a single risk score to a bank client based on many transactions. A common solution is to create features based on aggregates; calculating aggregates like min, max, sum, mean and count of the input level features and use these to predict the target. By using these aggregates, feature interaction on input level is lost which might result in suboptimal model performance. Moreover, model explainability is on aggregated feature level and does not reveal the importance at input level.\nTo overcome these issues, we need so called ‘aggregation learning’ models. During the Data Science for Finance Conference 2023, I presented how to implement a simple but powerful aggregation learning neural network for classification in Tensorflow. Please watch my presentation below."
  },
  {
    "objectID": "posts/optuna_spark.html",
    "href": "posts/optuna_spark.html",
    "title": "Ask-and-tell: How to use Optuna with Spark",
    "section": "",
    "text": "Optuna is a hyperparameter optimisation framework that implements a variety of efficient sampling algorithms. Although using Spark to distribute the optimisation task is not part of Optuna’s API, it’s relatively straightforward to do batch optimisation using the ‘ask-and-tell interface’. The Hyperopt Python package does have Spark support but it also requires Hyperopt to be installed on the Spark executors. Moreover, Optuna has some nice features that Hyperopt does not have. In this blogpost I show how to do use Optuna with Spark.\nFirst, we create a spark session and load the California housing dataset from scikit-learn that we will use for demonstation. To speed things up, we take a sample of 1,000 rows from this dataset.\n\nspark = SparkSession.builder.getOrCreate()\n\n\nfrom sklearn.datasets import fetch_california_housing\n\ndf = pd.concat(fetch_california_housing(as_frame=True, return_X_y=True), axis=1)\ndf = df.sample(n=1000, random_state=0)\n\ntarget = 'MedHouseVal'\nfeatures = df.drop(columns=[target]).columns.to_list()\n\nWe’re going to hyperparameter tune an XGBoost regression model. The hyperparameters that we tune are the learning rate, the maximum tree depth, the number of estimators and the L1 (alpha) en L2 (lambda) regularisation parameters. We create a function that takes a single set of hyperpameter values and that returns the mean of cross validation calculated scores. To calculate these, we use scikit-learn’s cross_validate function. By convention, ‘scores’ in scikit-learn require higher values to be better, so that’s why we need the negative version of the root mean squared error. This function will be run on the executors of the Spark cluster.\n\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_validate\n\n\ndef calculate_objective(learning_rate, max_depth, n_estimators, reg_alpha, reg_lambda):\n    estimator = XGBRegressor(learning_rate=learning_rate,\n                             max_depth=max_depth,\n                             n_estimators=n_estimators,\n                             reg_alpha=reg_alpha,\n                             reg_lambda=reg_lambda)\n    result = cross_validate(estimator, \n                            df[features], \n                            df[target], \n                            scoring='neg_root_mean_squared_error')\n    return result['test_score'].mean()\n\nNow we need to define the search space of the hyperparameters and parallelise the calculation. The Tree-structured Parzen Estimator is Optuna’s default sampler. A good exlanation on this algorithm can be found here. The first step in this algorithm is random sampling hyperparameter combinations and calculating the cost function for each. Setting the number of randomly sampled hyperparameters to a large value reduces the probability of ending up in a local optimum instead of the desired global optimum. As I’m running this locally, the ‘cluster’ contains only 8 executors so we set the batch size to 8. Let’s say we want to evaluate 50 batches of which the first 20 are to ‘startup’; these are randomly drawn hyperparameter combinations. As we use the negative mean squared error as cost function, we need to tell Optuna to maximize its value. Optuna documentation advises to set constant_liar to True for batch optimisation. This will avoid multiple executors to evaluate very similar hyperparameter combinations.\nWe provide all random hyperparameter combinations at once to the Spark cluster and not in batches of 8. This way, we don’t need to wait for the slowest of the 8 evaluations to be finished before continuing with the next set; all executors will be used until all random hyperparameter combinations are evaluated. To get hyperparameter combinations to be evaluated, the Study instance has an ask method. The result should be provided to the Study instance using the tell method. After the startup trials, we ask for new hyperparameter sets after each batch of 8 evaluations in order take advantage of the Optuna’s optimisation algorithm.\n\ndef objective_spark(trials):\n    params = [[t.suggest_float(\"learning_rate\", 0.01, 0.5),\n               t.suggest_int(\"max_depth\", 1, 100),\n               t.suggest_int(\"n_estimators\", 2, 1000),\n               t.suggest_float(\"reg_alpha\", 0, 10),\n               t.suggest_float(\"reg_lambda\", 0, 10),               \n              ] for t in trials]\n    rdd = spark.sparkContext.parallelize(params, len(params))\n    rdd_map_result = rdd.map(lambda p: calculate_objective(*p)).collect()\n    return rdd_map_result\n\n\nn_batches = 50\nbatch_size = 8\nn_startup_batches = 20\nn_startup_trials = n_startup_batches*batch_size\nstudy = optuna.create_study(\n    sampler=optuna.samplers.TPESampler(n_startup_trials=n_startup_trials,\n                                       constant_liar=True),\n    direction='maximize')\n\n# First get results for random hyperparameter combinations\nstartup_trials = [study.ask() for _ in range(n_startup_trials)]\nstartup_trial_numbers = [t.number for t in startup_trials]\nstartup_results = objective_spark(startup_trials)\n\nfor startup_trial_number, startup_result in zip(startup_trial_numbers, startup_results):\n    study.tell(startup_trial_number, startup_result)\nprint(\"After random parameter combinations:\")\nprint('Best parameters:', study.best_params, \"Best value:\", study.best_value)\n\n# Let the Tree-structured Parzen Estimator come up with combinations to try\nfor j in range(n_startup_batches, n_batches):\n    trials = [study.ask() for _ in range(batch_size)]\n    trial_numbers = [t.number for t in trials]\n    batch_results = objective_spark(trials)\n \n    for trial_number, batch_result in zip(trial_numbers, batch_results):\n        study.tell(trial_number, batch_result)\n    print(j, study.best_params, study.best_value)\n\n[I 2025-05-05 11:13:14,695] A new study created in memory with name: no-name-1d64bda7-4ba9-4d50-b0e1-6d9c264908b7\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [Stage 31:====================================&gt;                     (5 + 3) / 8]                                                                                \n\n\nAfter random parameter combinations:\nBest parameters: {'learning_rate': 0.1311691081810965, 'max_depth': 4, 'n_estimators': 664, 'reg_alpha': 1.6891693775759897, 'reg_lambda': 6.437762039469144} Best value: -0.5793585810589204\n20 {'learning_rate': 0.13104804160265776, 'max_depth': 3, 'n_estimators': 531, 'reg_alpha': 2.0165029530049488, 'reg_lambda': 5.766756295911177} -0.5758648464241053\n21 {'learning_rate': 0.08633253284746877, 'max_depth': 4, 'n_estimators': 467, 'reg_alpha': 2.1181398204415176, 'reg_lambda': 5.126682207615397} -0.5747320503447746\n22 {'learning_rate': 0.08633253284746877, 'max_depth': 4, 'n_estimators': 467, 'reg_alpha': 2.1181398204415176, 'reg_lambda': 5.126682207615397} -0.5747320503447746\n23 {'learning_rate': 0.08633253284746877, 'max_depth': 4, 'n_estimators': 467, 'reg_alpha': 2.1181398204415176, 'reg_lambda': 5.126682207615397} -0.5747320503447746\n24 {'learning_rate': 0.08633253284746877, 'max_depth': 4, 'n_estimators': 467, 'reg_alpha': 2.1181398204415176, 'reg_lambda': 5.126682207615397} -0.5747320503447746\n25 {'learning_rate': 0.08633253284746877, 'max_depth': 4, 'n_estimators': 467, 'reg_alpha': 2.1181398204415176, 'reg_lambda': 5.126682207615397} -0.5747320503447746\n26 {'learning_rate': 0.08633253284746877, 'max_depth': 4, 'n_estimators': 467, 'reg_alpha': 2.1181398204415176, 'reg_lambda': 5.126682207615397} -0.5747320503447746\n27 {'learning_rate': 0.08633253284746877, 'max_depth': 4, 'n_estimators': 467, 'reg_alpha': 2.1181398204415176, 'reg_lambda': 5.126682207615397} -0.5747320503447746\n28 {'learning_rate': 0.07879549009985837, 'max_depth': 4, 'n_estimators': 642, 'reg_alpha': 1.8414509615112093, 'reg_lambda': 5.6157124301959165} -0.5724592058132169\n29 {'learning_rate': 0.05979444899483358, 'max_depth': 4, 'n_estimators': 484, 'reg_alpha': 1.6687887588362003, 'reg_lambda': 5.999394259430553} -0.5721387540445764\n30 {'learning_rate': 0.05979444899483358, 'max_depth': 4, 'n_estimators': 484, 'reg_alpha': 1.6687887588362003, 'reg_lambda': 5.999394259430553} -0.5721387540445764\n31 {'learning_rate': 0.05979444899483358, 'max_depth': 4, 'n_estimators': 484, 'reg_alpha': 1.6687887588362003, 'reg_lambda': 5.999394259430553} -0.5721387540445764\n32 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n33 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n34 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n35 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n36 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n37 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n38 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n39 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n40 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n41 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n42 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n43 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n44 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n45 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n46 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n47 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n48 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n49 {'learning_rate': 0.0476114535610188, 'max_depth': 4, 'n_estimators': 473, 'reg_alpha': 1.580842456659082, 'reg_lambda': 5.086107195414869} -0.570046250869071\n\n\nLet’s now collect all results in a Pandas dataframe and investigate the results in a parallel coordinates plot.\n\nresult_df = pd.DataFrame([x.params for x in study.get_trials()])\nresult_df['test_score'] = [x.values[0] for x in study.get_trials()]\nresult_df['iteration'] = [x.number for x in study.get_trials()]\nresult_df = result_df.sort_values('test_score', ascending=False)\n\n\n\n\n\n\nlearning_rate\nmax_depth\nn_estimators\nreg_alpha\nreg_lambda\ntest_score\niteration\n\n\n\n\n0.047611\n4\n473\n1.580842\n5.086107\n-0.570046\n259\n\n\n0.055013\n4\n453\n1.637850\n4.717187\n-0.571130\n353\n\n\n0.054364\n4\n395\n1.201306\n4.987839\n-0.572038\n333\n\n\n0.059794\n4\n484\n1.668789\n5.999394\n-0.572139\n232\n\n\n0.040800\n4\n461\n1.912329\n5.103767\n-0.572192\n290\n\n\n\n\n\n\nimport plotly.express as px\n\n\npx.parallel_coordinates(result_df[['test_score', 'learning_rate',\n                                   'n_estimators', 'max_depth',\n                                   'reg_alpha', 'reg_lambda']],\n                        color='test_score')\n\n\n        \n        \n        \n\n\n\n                            \n                                            \n\n\nWhen we include the iteration number in the parallel coordinates plot, we can confirm that the first 20*8=160 iterations were totally random and nicely covered the whole parameter space. Afterwards we see how the algorithm converges towards the optimal hyperparameter combination. The plotly parallel coordinates plot allows for highlighting a subset of values by selecting ranges on the individual axes. I show this in the animation below.\n\npx.parallel_coordinates(result_df[['iteration', 'learning_rate',\n                                   'n_estimators', 'max_depth',\n                                   'reg_alpha', 'reg_lambda']],\n                        color='iteration')"
  },
  {
    "objectID": "posts/Deduplipy.html",
    "href": "posts/Deduplipy.html",
    "title": "Deduplication of records using DedupliPy",
    "section": "",
    "text": "Deduplication or entity resolution is the task to combine different representations of the same real world entity. The Python package DedupliPy implements deduplication using active learning. Active learning allows for rapid training without having to provide a large, manually labelled dataset. In this post I demonstrate how the package works and show more advanced settings. In case you want to apply entity resolution on large data in Spark, please have a look at Spark-Matcher, a package I developed together with two colleagues."
  },
  {
    "objectID": "posts/Deduplipy.html#installation",
    "href": "posts/Deduplipy.html#installation",
    "title": "Deduplication of records using DedupliPy",
    "section": "Installation",
    "text": "Installation\nDedupliPy can simply be installed from PyPi. Just type the following in the command line:\npip install deduplipy"
  },
  {
    "objectID": "posts/Deduplipy.html#simple-deduplication",
    "href": "posts/Deduplipy.html#simple-deduplication",
    "title": "Deduplication of records using DedupliPy",
    "section": "Simple deduplication",
    "text": "Simple deduplication\nDedupliPy comes with example data. We first load the ‘voters’ data that contains duplicate records:\n\nfrom deduplipy.datasets import load_data\n\ndf = load_data(kind='voters')\n\nColumn names: 'name', 'suburb', 'postcode'\n\n\nThis dataset contains names, suburbs and postcodes.\n\n\n\n\n\nname\nsuburb\npostcode\n\n\n\n\nkhimerc thomas\ncharlotte\n2826g\n\n\nlucille richardst\nkannapolis\n28o81\n\n\nreb3cca bauerboand\nraleigh\n27615\n\n\n\n\n\nCreate a Deduplicator instance and provide the column names to be used for deduplication:\n\nfrom deduplipy.deduplicator import Deduplicator\n\n\nmyDedupliPy = Deduplicator(['name', 'suburb', 'postcode'])\n\nFit the Deduplicator by active learning; enter whether a pair is a match (y) or not (n). When the training is converged, you will be notified and you can finish training by entering ‘f’.\n\nmyDedupliPy.fit(df)\n\nApply the trained Deduplicator on (new) data. The column deduplication_id is the identifier for a cluster. Rows with the same deduplication_id are found to be the same real world entity.\n\nres = myDedupliPy.predict(df)\n\n\n\n\n\n\nname\nsuburb\npostcode\ndeduplication_id\n\n\n\n\ncaria macartney\ncharlotte\n28220\n1\n\n\ncarla macartney\ncharlotte\n28227\n1\n\n\nmartha safrit\ncha4lotte\n282l5\n2\n\n\nmartha safrit\ncharlotte\n28215\n2\n\n\njeanronel corbier\ncharlotte\n28213\n3\n\n\njeanronel corpier\ncharrlotte\n28213\n3\n\n\nmelissa kaltenbach\ncharlotte\n28211\n4\n\n\nmelissa kalteribach\ncharlotte\n28251\n4\n\n\nkiea matthews\ncharlotte\n28218\n5\n\n\nkiera matthews\ncharlotte\n28216\n5\n\n\n\n\n\nThe Deduplicator instance can be saved as a pickle file and be applied on new data after training:\n\nimport pickle\n\n\nwith open('mypickle.pkl', 'wb') as f:\n    pickle.dump(myDedupliPy, f)\n\n\nwith open('mypickle.pkl', 'rb') as f:\n    loaded_obj = pickle.load(f)\n\n\nres = loaded_obj.predict(df)\n\n\n\n\n\n\nname\nsuburb\npostcode\ndeduplication_id\n\n\n\n\ncaria macartney\ncharlotte\n28220\n1\n\n\ncarla macartney\ncharlotte\n28227\n1\n\n\nmartha safrit\ncha4lotte\n282l5\n2\n\n\nmartha safrit\ncharlotte\n28215\n2\n\n\njeanronel corbier\ncharlotte\n28213\n3\n\n\njeanronel corpier\ncharrlotte\n28213\n3\n\n\nmelissa kaltenbach\ncharlotte\n28211\n4\n\n\nmelissa kalteribach\ncharlotte\n28251\n4\n\n\nkiea matthews\ncharlotte\n28218\n5\n\n\nkiera matthews\ncharlotte\n28216\n5"
  },
  {
    "objectID": "posts/Deduplipy.html#advanced-deduplication",
    "href": "posts/Deduplipy.html#advanced-deduplication",
    "title": "Deduplication of records using DedupliPy",
    "section": "Advanced deduplication",
    "text": "Advanced deduplication\nIf you’re intested in the inner workings of DedupliPy, please watch my presentation at PyData Global 2021:\n\nLet’s explore some advanced settings to tailor the deduplicator to our needs. We are going to select the similarity metrics per field, define our own blocking rules and include interaction between the fields.\nThe similarity metrics per field are entered in a dict. Similarity metric can be any function that takes two strings and output a number. We use some string similarity functions that are implemented in the Python package called ‘thefuzz’ (pip install thefuzz):\n\nfrom thefuzz.fuzz import ratio, partial_ratio, token_set_ratio, token_sort_ratio\n\n\nfield_info = {'name':[ratio, partial_ratio], \n              'suburb':[token_set_ratio, token_sort_ratio], \n              'postcode':[ratio]}\n\nWe choose a set of rules for blocking which we define ourselves. We only apply this rule to the ‘name’ column.\n\ndef first_two_characters(x):\n    return x[:2]\n\nWhen we setinteraction=True, the classifier includes interaction features, e.g. ratio('name') * token_set_ratio('suburb'). When interaction features are included, the logistic regression classifier applies a L1 regularisation to prevent overfitting. We also set verbose=1 to get information on the progress and a distribution of scores\n\nmyDedupliPy = Deduplicator(field_info=field_info, interaction=True, \n                           rules={'name': [first_two_characters]}, verbose=1)\n\nFit the Deduplicator by active learning; enter whether a pair is a match (y) or not (n). When the training is converged, you will be notified and you can finish training by entering ‘f’.\n\nmyDedupliPy.fit(df)\n\nAfter fitting, the histogram of scores is shown. Based on this histogram, we decide to ignore all pairs with a similarity probability lower than 0.1 when predicting:\nApply the trained Deduplicator on (new) data. The column deduplication_id is the identifier for a cluster. Rows with the same deduplication_id are found to be the same real world entity.\n\nres = myDedupliPy.predict(df, score_threshold=0.1)\n\n\n\n\n\n\nname\nsuburb\npostcode\ndeduplication_id\n\n\n\n\nlucille richardst\nkannapolis\n28o81\n1\n\n\nlucille richards\nkannapolis\n28081\n1\n\n\nlutta baldwin\nwhiteville\n28472\n3\n\n\nlutta baldwin\nwhitevill\n28475\n3\n\n\nrepecca harrell\nwinton\n27q86\n5\n\n\nrebecca harrell\nwinton\n27986\n5\n\n\nrebecca harrell\nwitnon\n27926\n5\n\n\nrebecca bauerband\nraleigh\n27615\n6\n\n\nreb3cca bauerboand\nraleigh\n27615\n6\n\n\nrebeccah shelton\nwhittier\n28789\n7"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data science blog",
    "section": "",
    "text": "Welcome to my data science blog.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nAggregation learning\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAsk-and-tell: How to use Optuna with Spark\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDeduplication of records using DedupliPy\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDistributed hyperparameter tuning of Scikit-learn models in Spark\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFinding duplicate records using PyMinHash\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHard-to-classify datapoints in imbalanced data problems\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nTaxonomy feature encoding\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ripper_text_features.html",
    "href": "ripper_text_features.html",
    "title": "Rule mining with text features using Ripper",
    "section": "",
    "text": "pip install wittgenstein\nKaggle Wine Reviews dataset\nripper\nexplanation\n\n\n\n\n\n\n\n\n\ndescription\ncountry\nvariety\nprice\nexcellent\n\n\n\n\n74061\nOld-gold in color, this wine has concentrated flavors with an almost medicinal intensity. Old wo...\nPortugal\nPort\n200.0\n1\n\n\n98873\nAlready a modern classic of the Douro, this red wine from Niepoort, with its baked apple and bla...\nPortugal\nPortuguese Red\n27.0\n0\n\n\n144736\nMy experience of Swan Pinots is that they've been made the same way for a long time, and require...\nUS\nPinot Noir\n45.0\n1\n\n\n138479\nIn keeping with the austerity of the Dão landscape, this is a taut, mineral-inspired wine, with ...\nPortugal\nPortuguese Red\n9.0\n0\n\n\n114502\nOpens with a lovely bouquet of lemon and tart pineapple fruit with anise, mineral and tobacco ac...\nUS\nChardonnay\n45.0\n0\n\n\n\n\n\n\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer().fit(df['description'])\n\nlen(cv.vocabulary_)\n\n4299\n\n\n\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\n\nclass TopTokenTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self, col, top_n=5, vectorizer_kwargs={}):\n        self.col = col\n        self.top_n = top_n\n        self.vectorizer_kwargs = vectorizer_kwargs\n        \n    @staticmethod\n    def get_top_tokens(feature_names, coefs, top_n):\n        # tokens = pd.DataFrame({'token': feature_names, 'coef':coefs})\n        # tokens['abs_coef'] = tokens['coef'].abs()\n        # return (tokens.sort_values('abs_coef', ascending=False)\n        #         .head(top_n)['token'].tolist())\n        return [x for x, _ in sorted(zip(feature_names, coefs), key=lambda pair: abs(pair[1]), reverse=True)]\n    \n    def fit(self, X, y):\n        pipe = make_pipeline(CountVectorizer(**self.vectorizer_kwargs),\n                            LogisticRegression(class_weight='balanced', max_iter=1000))\n        pipe.fit(X[self.col],y)\n        vec = pipe.steps[0][1]\n        clf = pipe.steps[1][1]\n        feature_names = vec.get_feature_names_out()\n        coefs = clf.coef_[0]\n        self.top_tokens_ = TopTokenTransformer.get_top_tokens(feature_names, coefs, self.top_n)\n        self.vec_top = CountVectorizer(vocabulary=self.top_tokens_, **self.vectorizer_kwargs)\n        return self\n    \n    def transform(self, X):\n        return self.vec_top.transform(X[self.col]).toarray()\n    \n    def get_feature_names_out(self, input_features=None):\n        return self.top_tokens_\n\n\ndescription: text\ncategorical:\n\ncountry\nvariety\n\nprice: numeric\n\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\ntransformer = (ColumnTransformer([\n    ('toptokens', TopTokenTransformer('description', top_n=10), ['description']),\n    ('categorical', OneHotEncoder(max_categories=5, sparse_output=False), ['country', 'variety']),\n    ('numeric', 'passthrough', ['price'])])\n               .set_output(transform='pandas'))  # needed to pass feature names to ripper\n\n\nfrom wittgenstein import RIPPER\n\npipe = make_pipeline(transformer, RIPPER(random_state=0))\n\npipe.fit(df[['description', 'country', 'variety', 'price']], df['excellent'])\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('toptokens',\n                                                  TopTokenTransformer(col='description',\n                                                                      top_n=10),\n                                                  ['description']),\n                                                 ('categorical',\n                                                  OneHotEncoder(max_categories=5,\n                                                                sparse_output=False),\n                                                  ['country', 'variety']),\n                                                 ('numeric', 'passthrough',\n                                                  ['price'])])),\n                ('ripper',\n                 &lt;RIPPER(dl_allowance=64, random_state=0, prune_size=0.33, max_rules=None, max_total_conds=None, k=2, verbosity=0, max_rule_conds=None, n_discretize_bins=10, alpha=1.0) with fit ruleset&gt;)])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('toptokens',\n                                                  TopTokenTransformer(col='description',\n                                                                      top_n=10),\n                                                  ['description']),\n                                                 ('categorical',\n                                                  OneHotEncoder(max_categories=5,\n                                                                sparse_output=False),\n                                                  ['country', 'variety']),\n                                                 ('numeric', 'passthrough',\n                                                  ['price'])])),\n                ('ripper',\n                 &lt;RIPPER(dl_allowance=64, random_state=0, prune_size=0.33, max_rules=None, max_total_conds=None, k=2, verbosity=0, max_rule_conds=None, n_discretize_bins=10, alpha=1.0) with fit ruleset&gt;)])columntransformer: ColumnTransformerColumnTransformer(transformers=[('toptokens',\n                                 TopTokenTransformer(col='description',\n                                                     top_n=10),\n                                 ['description']),\n                                ('categorical',\n                                 OneHotEncoder(max_categories=5,\n                                               sparse_output=False),\n                                 ['country', 'variety']),\n                                ('numeric', 'passthrough', ['price'])])toptokens['description']TopTokenTransformerTopTokenTransformer(col='description', top_n=10)categorical['country', 'variety']OneHotEncoderOneHotEncoder(max_categories=5, sparse_output=False)numeric['price']passthroughpassthroughRIPPER&lt;RIPPER(dl_allowance=64, random_state=0, prune_size=0.33, max_rules=None, max_total_conds=None, k=2, verbosity=0, max_rule_conds=None, n_discretize_bins=10, alpha=1.0)&gt;\n\n\n\nrip = pipe.steps[-1][1]\nrip.selected_features_\n\n['numeric__price',\n 'categorical__variety_Cabernet Sauvignon',\n 'categorical__country_Italy',\n 'toptokens__elegant',\n 'categorical__country_US',\n 'toptokens__rich',\n 'toptokens__through',\n 'toptokens__complex',\n 'toptokens__intense',\n 'toptokens__structure']\n\n\n\nrip = pipe.steps[-1][1]\nrip.selected_features_\n\n['numeric__price',\n 'toptokens__for',\n 'toptokens__the',\n 'toptokens__rich',\n 'toptokens__now',\n 'toptokens__complex',\n 'toptokens__great',\n 'toptokens__berry',\n 'toptokens__aromas',\n 'toptokens__mouth',\n 'toptokens__elegant']\n\n\n\nrip.out_model()\n\n[[numeric__price=&gt;60.0 ^ categorical__country_US=0.0 ^ categorical__country_France=1.0 ^ categorical__variety_Chardonnay=1.0 ^ toptokens__glorious=1] V\n[numeric__price=&gt;60.0 ^ categorical__country_US=0.0 ^ categorical__country_France=1.0 ^ categorical__variety_Chardonnay=1.0] V\n[numeric__price=&gt;60.0 ^ categorical__variety_CabernetSauvignon=0.0 ^ categorical__country_France=1.0 ^ toptokens__2025=1] V\n[numeric__price=&gt;60.0 ^ categorical__variety_CabernetSauvignon=0.0 ^ categorical__country_France=1.0 ^ categorical__variety_infrequent_sklearn=0.0 ^ categorical__variety_PinotNoir=0.0] V\n[numeric__price=&gt;60.0 ^ categorical__variety_CabernetSauvignon=0.0 ^ categorical__country_France=1.0 ^ toptokens__93=1] V\n[numeric__price=&gt;60.0 ^ categorical__variety_CabernetSauvignon=0.0 ^ categorical__country_France=1.0] V\n[numeric__price=&gt;60.0 ^ categorical__variety_CabernetSauvignon=0.0 ^ categorical__country_Italy=1.0 ^ categorical__variety_infrequent_sklearn=0.0 ^ categorical__variety_Chardonnay=1.0] V\n[numeric__price=&gt;60.0 ^ categorical__variety_CabernetSauvignon=0.0 ^ categorical__country_Italy=1.0 ^ categorical__variety_RedBlend=1.0] V\n[numeric__price=&gt;60.0 ^ categorical__variety_CabernetSauvignon=0.0 ^ categorical__country_Italy=1.0 ^ toptokens__glorious=1] V\n[numeric__price=&gt;60.0 ^ categorical__variety_CabernetSauvignon=0.0 ^ categorical__country_Italy=1.0] V\n[numeric__price=42.0-60.0 ^ categorical__country_Italy=1.0 ^ categorical__variety_infrequent_sklearn=0.0 ^ categorical__variety_RedBlend=0.0] V\n[numeric__price=42.0-60.0 ^ categorical__country_US=0.0 ^ categorical__country_France=0.0 ^ categorical__country_Spain=0.0 ^ categorical__variety_infrequent_sklearn=1.0 ^ categorical__country_Italy=0.0 ^ toptokens__2025=1] V\n[numeric__price=42.0-60.0 ^ categorical__country_Italy=1.0 ^ categorical__variety_RedBlend=1.0] V\n[numeric__price=42.0-60.0 ^ categorical__variety_CabernetSauvignon=0.0 ^ categorical__country_infrequent_sklearn=1.0 ^ categorical__variety_infrequent_sklearn=1.0] V\n[numeric__price=42.0-60.0 ^ categorical__country_Italy=1.0] V\n[numeric__price=&gt;60.0 ^ categorical__variety_CabernetSauvignon=0.0 ^ categorical__variety_RedBlend=0.0 ^ categorical__country_infrequent_sklearn=1.0 ^ categorical__variety_PinotNoir=0.0 ^ toptokens__2025=1] V\n[numeric__price=&gt;60.0 ^ categorical__variety_CabernetSauvignon=0.0 ^ categorical__variety_RedBlend=0.0 ^ categorical__variety_infrequent_sklearn=0.0 ^ categorical__variety_Chardonnay=1.0] V\n[numeric__price=&gt;60.0 ^ categorical__variety_CabernetSauvignon=0.0 ^ categorical__variety_PinotNoir=1.0 ^ categorical__country_US=1.0] V\n[numeric__price=42.0-60.0 ^ categorical__variety_PinotNoir=1.0 ^ categorical__country_US=1.0 ^ toptokens__2025=1] V\n[numeric__price=42.0-60.0 ^ categorical__variety_PinotNoir=1.0 ^ categorical__country_US=1.0] V\n[numeric__price=&gt;60.0 ^ categorical__variety_infrequent_sklearn=1.0 ^ categorical__country_infrequent_sklearn=1.0] V\n[numeric__price=34.0-42.0 ^ categorical__country_US=0.0 ^ categorical__variety_infrequent_sklearn=1.0 ^ categorical__country_infrequent_sklearn=1.0 ^ toptokens__2025=1] V\n[numeric__price=42.0-60.0 ^ categorical__variety_Chardonnay=1.0 ^ categorical__country_France=0.0 ^ categorical__country_US=1.0] V\n[numeric__price=34.0-42.0 ^ categorical__country_US=0.0 ^ categorical__variety_infrequent_sklearn=1.0] V\n[numeric__price=&gt;60.0] V\n[numeric__price=32.0-34.0 ^ categorical__country_France=1.0 ^ toptokens__92=1] V\n[numeric__price=42.0-60.0 ^ categorical__variety_infrequent_sklearn=1.0 ^ categorical__country_France=1.0 ^ toptokens__2025=1] V\n[numeric__price=42.0-60.0 ^ categorical__country_France=1.0 ^ categorical__variety_infrequent_sklearn=1.0] V\n[numeric__price=42.0-60.0] V\n[numeric__price=32.0-34.0 ^ categorical__country_France=1.0 ^ toptokens__91=1] V\n[numeric__price=34.0-42.0 ^ categorical__variety_Chardonnay=1.0 ^ categorical__country_US=1.0] V\n[numeric__price=32.0-34.0 ^ categorical__country_France=1.0 ^ toptokens__93=1] V\n[numeric__price=34.0-42.0 ^ categorical__country_Italy=1.0 ^ categorical__variety_Chardonnay=1.0] V\n[numeric__price=34.0-42.0 ^ categorical__country_Italy=1.0 ^ categorical__variety_CabernetSauvignon=1.0] V\n[numeric__price=34.0-42.0 ^ categorical__country_infrequent_sklearn=0.0 ^ categorical__country_Italy=1.0] V\n[numeric__price=32.0-34.0 ^ categorical__country_France=1.0 ^ toptokens__88=0 ^ toptokens__87=0 ^ categorical__variety_PinotNoir=1.0] V\n[numeric__price=34.0-42.0 ^ categorical__country_US=1.0 ^ toptokens__2025=1] V\n[numeric__price=32.0-34.0 ^ categorical__country_France=1.0 ^ toptokens__88=0 ^ toptokens__87=0 ^ categorical__variety_Chardonnay=1.0] V\n[numeric__price=34.0-42.0 ^ categorical__country_US=1.0 ^ categorical__variety_infrequent_sklearn=1.0] V\n[numeric__price=32.0-34.0 ^ categorical__country_France=1.0 ^ toptokens__88=0 ^ toptokens__87=0 ^ categorical__variety_infrequent_sklearn=1.0] V\n[numeric__price=26.0-32.0 ^ categorical__country_infrequent_sklearn=1.0 ^ categorical__variety_infrequent_sklearn=1.0 ^ toptokens__88=0] V\n[numeric__price=26.0-32.0 ^ categorical__variety_PinotNoir=0.0 ^ categorical__variety_Chardonnay=1.0 ^ categorical__country_US=1.0 ^ toptokens__glycerin=0] V\n[numeric__price=26.0-32.0 ^ categorical__country_US=0.0 ^ categorical__variety_PinotNoir=0.0 ^ categorical__country_France=1.0 ^ categorical__variety_infrequent_sklearn=1.0] V\n[numeric__price=34.0-42.0 ^ categorical__country_US=1.0 ^ categorical__variety_PinotNoir=1.0] V\n[numeric__price=32.0-34.0 ^ categorical__country_France=0.0 ^ categorical__country_infrequent_sklearn=1.0 ^ toptokens__2025=1] V\n[numeric__price=32.0-34.0 ^ categorical__country_France=0.0 ^ categorical__country_infrequent_sklearn=1.0 ^ categorical__variety_PinotNoir=0.0 ^ categorical__variety_CabernetSauvignon=1.0] V\n[numeric__price=32.0-34.0 ^ categorical__country_France=0.0 ^ categorical__variety_RedBlend=1.0 ^ categorical__country_Italy=1.0] V\n[numeric__price=26.0-32.0 ^ categorical__variety_PinotNoir=0.0 ^ categorical__country_US=0.0 ^ categorical__variety_RedBlend=1.0 ^ categorical__country_Italy=1.0] V\n[numeric__price=26.0-32.0 ^ categorical__variety_PinotNoir=0.0 ^ categorical__country_infrequent_sklearn=1.0 ^ categorical__variety_Chardonnay=1.0]]\n\n\n\n\n\n\n\n\n\n\n\ndescription\nexcellent\n\n\n\n\n144700\n90-92 Barrel sample. Very citrusy fruits, with spice, new wood and delicious, almost tropical fl...\n1\n\n\n144701\n90-92 Barrel sample. A ripe, creamy wine, which has great poise and freshness. There's just a to...\n1\n\n\n144711\n90-92 Barrel sample. Large-scale renovations to the vineyard and the advice of Patrick Valette a...\n1\n\n\n144712\n90-92 Barrel sample. A hugely ripe wine with dark tannins, this wine broods with intense, tough ...\n1\n\n\n146194\nAs with the Merlot this is Ash Hollow fruit (92%), hence young; it seems to represent a signific...\n0\n\n\n\n\n\n\n\n\ntransformer = (ColumnTransformer([\n    ('toptokens', TopTokenTransformer('description', top_n=10, \n                        vectorizer_kwargs={'token_pattern': r'\\b[a-zA-Z]+\\b'}), ['description']),\n    ('categorical', OneHotEncoder(max_categories=5, sparse_output=False), ['country', 'variety']),\n    ('numeric', 'passthrough', ['price'])])\n               .set_output(transform='pandas'))\n\n\npipe = make_pipeline(transformer, RIPPER(random_state=0))\n\npipe.fit(df[['description', 'country', 'variety', 'price']], df['excellent'])\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('toptokens',\n                                                  TopTokenTransformer(col='description',\n                                                                      top_n=10,\n                                                                      vectorizer_kwargs={'token_pattern': '\\\\b[a-zA-Z]+\\\\b'}),\n                                                  ['description']),\n                                                 ('categorical',\n                                                  OneHotEncoder(max_categories=5,\n                                                                sparse_output=False),\n                                                  ['country', 'variety']),\n                                                 ('numeric', 'passthrough',\n                                                  ['price'])])),\n                ('ripper',\n                 &lt;RIPPER(random_state=0, verbosity=0, n_discretize_bins=10, alpha=1.0, max_rules=None, k=2, dl_allowance=64, max_total_conds=None, max_rule_conds=None, prune_size=0.33) with fit ruleset&gt;)])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('toptokens',\n                                                  TopTokenTransformer(col='description',\n                                                                      top_n=10,\n                                                                      vectorizer_kwargs={'token_pattern': '\\\\b[a-zA-Z]+\\\\b'}),\n                                                  ['description']),\n                                                 ('categorical',\n                                                  OneHotEncoder(max_categories=5,\n                                                                sparse_output=False),\n                                                  ['country', 'variety']),\n                                                 ('numeric', 'passthrough',\n                                                  ['price'])])),\n                ('ripper',\n                 &lt;RIPPER(random_state=0, verbosity=0, n_discretize_bins=10, alpha=1.0, max_rules=None, k=2, dl_allowance=64, max_total_conds=None, max_rule_conds=None, prune_size=0.33) with fit ruleset&gt;)])columntransformer: ColumnTransformerColumnTransformer(transformers=[('toptokens',\n                                 TopTokenTransformer(col='description',\n                                                     top_n=10,\n                                                     vectorizer_kwargs={'token_pattern': '\\\\b[a-zA-Z]+\\\\b'}),\n                                 ['description']),\n                                ('categorical',\n                                 OneHotEncoder(max_categories=5,\n                                               sparse_output=False),\n                                 ['country', 'variety']),\n                                ('numeric', 'passthrough', ['price'])])toptokens['description']TopTokenTransformerTopTokenTransformer(col='description', top_n=10,\n                    vectorizer_kwargs={'token_pattern': '\\\\b[a-zA-Z]+\\\\b'})categorical['country', 'variety']OneHotEncoderOneHotEncoder(max_categories=5, sparse_output=False)numeric['price']passthroughpassthroughRIPPER&lt;RIPPER(random_state=0, verbosity=0, n_discretize_bins=10, alpha=1.0, max_rules=None, k=2, dl_allowance=64, max_total_conds=None, max_rule_conds=None, prune_size=0.33)&gt;\n\n\n\nrip = pipe.steps[-1][1]\nrip.selected_features_\n\n['numeric__price',\n 'categorical__country_US',\n 'categorical__variety_infrequent_sklearn',\n 'categorical__country_infrequent_sklearn',\n 'toptokens__greece',\n 'categorical__variety_Chardonnay',\n 'toptokens__sample',\n 'categorical__country_France',\n 'categorical__country_Italy',\n 'categorical__variety_Red Blend',\n 'categorical__variety_Cabernet Sauvignon',\n 'categorical__variety_Pinot Noir']\n\n\n\nrip.out_model()\n\n[[numeric__price=&gt;60.0] V\n[numeric__price=42.0-60.0] V\n[numeric__price=34.0-42.0 ^ categorical__country_US=0.0 ^ categorical__variety_infrequent_sklearn=1.0 ^ categorical__country_infrequent_sklearn=1.0 ^ toptokens__greece=0] V\n[numeric__price=34.0-42.0 ^ categorical__variety_Chardonnay=1.0 ^ categorical__country_US=1.0] V\n[numeric__price=32.0-34.0 ^ toptokens__sample=1 ^ categorical__country_France=1.0] V\n[numeric__price=34.0-42.0 ^ categorical__country_Italy=1.0 ^ categorical__variety_infrequent_sklearn=0.0 ^ categorical__variety_RedBlend=0.0 ^ categorical__variety_CabernetSauvignon=1.0] V\n[numeric__price=34.0-42.0 ^ categorical__country_Italy=1.0 ^ categorical__variety_infrequent_sklearn=1.0] V\n[numeric__price=34.0-42.0 ^ categorical__country_France=1.0 ^ categorical__variety_infrequent_sklearn=1.0] V\n[numeric__price=34.0-42.0 ^ categorical__country_Italy=1.0 ^ categorical__variety_Chardonnay=1.0] V\n[numeric__price=34.0-42.0 ^ categorical__variety_PinotNoir=0.0 ^ categorical__country_Italy=1.0] V\n[numeric__price=34.0-42.0 ^ categorical__country_US=1.0 ^ categorical__variety_PinotNoir=1.0]]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I work as a data scientist for a financial institution. My main topics of interest are entity resolution, fuzzy matching, classification for imbalanced data problems and aggregation learning.\nSome of the libraries I created or co-created:\n\nDeduplipy - Entity resolution package (deduplipy.com, GitHub, PyData Global presentation)\n\nSpark-Matcher - Entity resolution and fuzzy matching at scale in Spark (GitHub)\nPyMinHash - Minhashing in Python (GitHub)\nOther:\n\nLockdownRadar.nl (newspaper article)\n\nPodcast interview on ChatGPT (in Dutch)\n\nScikit-learn certified"
  },
  {
    "objectID": "about.html#frits-hermans",
    "href": "about.html#frits-hermans",
    "title": "About",
    "section": "",
    "text": "I work as a data scientist for a financial institution. My main topics of interest are entity resolution, fuzzy matching, classification for imbalanced data problems and aggregation learning.\nSome of the libraries I created or co-created:\n\nDeduplipy - Entity resolution package (deduplipy.com, GitHub, PyData Global presentation)\n\nSpark-Matcher - Entity resolution and fuzzy matching at scale in Spark (GitHub)\nPyMinHash - Minhashing in Python (GitHub)\nOther:\n\nLockdownRadar.nl (newspaper article)\n\nPodcast interview on ChatGPT (in Dutch)\n\nScikit-learn certified"
  }
]